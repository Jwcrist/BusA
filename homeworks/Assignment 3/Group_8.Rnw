\documentclass{article}
\usepackage[vmargin=1in,hmargin=1in]{geometry}
\usepackage{enumerate}
\begin{document}
\title{Homework 3}
\date{BSAD 8700 - Business Analytics\\ Due: February 2, 2015}
\author{Kris Hanus, Laura Glathar, Arkya Rakshit, Jace Crist, Brandon Dlugosz\\ University of Nebraska at Omaha}
\maketitle

\textbf{ANSWER FOR 8:} \\
<<echo=FALSE, warning=FALSE>>=
# install.packages("ISLR", "MASS")
library(MASS)
library(ISLR)
library(ggplot2)
@

\begin{enumerate}[(a)]
\item
<<>>=
head(Auto,3)
@
\begin{enumerate}[(i)]
\item
<<>>=
predict1<-lm(mpg~horsepower, data=Auto)
summary(predict1)
@
There is a very strong relationship between the predictor and the response variable. However, according to the hypothesis we would suggest that there might be a transformation that would allow us to create a better prediction. This can be seen from plotting that the Beta values may not be under the assumption of linearity.

<<>>=
ggplot(Auto, aes(horsepower, mpg))+geom_point()
@

\item
<<>>=
fvalue<-qf(.95,390,1)
fvalue
@
The Adjusted R-Squared value would suggest that a linear line would fit the data fairly well, but as the F-statistic shows and p-value supports that the fit is significant. However, as can be shown by graph or residuals this is a curvi-linear plot. We would need either a transformation or an adjusted curvi-linear independent value.

\item

<<>>=
cor(Auto[1:8])
@
There is a negative relationship

\item
<<>>=

NewDat<-data.frame(horsepower=98)
predict(predict1, NewDat,interval = "confidence", level=0.95, se.fit=FALSE)
predict(predict1, NewDat,interval = "prediction", level=0.95, se.fit=FALSE)
@
\end{enumerate}
\item
<<>>=
ggplot(Auto, aes(horsepower,mpg))+
  geom_point()+
  geom_abline(intercept=39.93, slope=-0.157845)
@

\item
Extra Credit:
<<>>=
ggplot(Auto, aes(horsepower,mpg))+geom_point()+
  geom_abline(intercept=39.93, slope=-0.157845)+
  geom_smooth(method="lm", se=T)
@
The fitted line goes through the mean. It is evident that the residuals of the points (from left to right) would start off positive, would then become negative, and will finish positive. By looking at this trend we would suggest that the data is either logistic or $\frac{1}{x}$ and there might be a better fit model to the data.
\end{enumerate}

\textbf{ANSWER FOR 9:} \\
\begin{enumerate}[(a)]

\item
<<>>=
pairs(Auto)
@

\item
<<>>=
str(Auto)
cor(Auto[1:8])
@

\item
\begin{enumerate}[(i)]
\item
<<>>=
lm.fit<-lm(mpg~.,data=Auto[1:8])
summary(lm.fit)
lm.fit<-lm(mpg~.-acceleration-cylinders-horsepower-displacement,data=Auto[1:8])
summary(lm.fit)
@
There is some relationship between predictors and the response variable.
\item
 The variables which have the highest correlation would be weight, year, and origin. They are all significant to within $99.9\%$. There were two other variables (horsepower and displacement) that would have been significant together to a $90\%$, however; we are looking for correlations above $95\%$ and preventing over-fitting the model they were eliminated.
\item
The $\beta_{year}$ is equal to $.7571$. This is a positive correlation between year and mpg.
\end{enumerate}
\item
Extra Credit
\item
Extra Credit
\item
Extra Credit


\end{enumerate}
\end{document}